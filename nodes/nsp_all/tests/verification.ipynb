{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify data between new node and old streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do verification:\n",
    "- run the nsp_all_verify_graph.yaml for 1-2min\n",
    "- make sure the parameters are same across old pipeline and new one\n",
    "- stop graph without running the saveRDB derivative\n",
    "- run this notebook in rt env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary cache directory at /tmp/matplotlib-34c9_q5s because the default path (/home/pdeevi/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from redis import  Redis\n",
    "from brand.redis import xread_sync\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "redis_host = '192.168.30.6'\n",
    "redis_port = 27263\n",
    "redis_socket = 10\n",
    "\n",
    "r = Redis(redis_host, redis_port, retry_on_timeout=True)\n",
    "r.ping()\n",
    "\n",
    "supergraph_entry = r.xrevrange(b'supergraph_stream', '+', '-', 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33167 33033\n",
      "331680 330337\n",
      "331676 330337\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(r.xlen(\"binned_spikes_old\"),r.xlen(\"binned_spikes_all\"))\n",
    "print(r.xlen(\"reref_neural_old\"),r.xlen(\"reref_neural_all\"))\n",
    "print(r.xlen(\"thresh_cross_old\"),r.xlen(\"thresh_cross_all\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 3000\n",
    "rn_all = r.xread({\"reref_neural_all\": b'0-0'}, count=count)[0][1]\n",
    "rn_old = r.xread({\"reref_neural_old\": b'0-0'}, count=count)[0][1]\n",
    "tx_all = r.xread({\"thresh_cross_all\": b'0-0'}, count=count)[0][1]\n",
    "tx_old = r.xread({\"thresh_cross_old\": b'0-0'}, count=count)[0][1]\n",
    "sb_all = r.xread({\"sbp_all\": b'0-0'}, count=count)[0][1]\n",
    "sb_old = r.xread({\"sbp_old\": b'0-0'}, count=count)[0][1]\n",
    "bs_all = r.xread({\"binned_spikes_all\": b'0-0'}, count=count)[0][1]\n",
    "bs_old = r.xread({\"binned_spikes_old\": b'0-0'}, count=count)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected num of matches 1661\n"
     ]
    }
   ],
   "source": [
    "nsp_idx_all = np.array([json.loads(tx_all_i[1][b'sync'].decode())['nsp_idx_1'] for tx_all_i in tx_all])\n",
    "nsp_idx_old = np.array([json.loads(tx_old_i[1][b'sync'].decode())['nsp_idx_1'] for tx_old_i in tx_old])\n",
    "print(f\"Expected num of matches {count - (nsp_idx_all[0] - nsp_idx_old[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected num of matches 1661\n"
     ]
    }
   ],
   "source": [
    "nsp_idx_all = np.array([json.loads(tx_all_i[1][b'sync'].decode())['nsp_idx_1'] for tx_all_i in sb_all])\n",
    "nsp_idx_old = np.array([json.loads(tx_old_i[1][b'sync'].decode())['nsp_idx_1'] for tx_old_i in sb_old])\n",
    "print(f\"Expected num of matches {count - (nsp_idx_all[0] - nsp_idx_old[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1657"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "matches = []\n",
    "for i,rn_all_i in enumerate(rn_all):\n",
    "    for j,rn_old_i in enumerate(rn_old):\n",
    "        if rn_all_i[1][b'samples'] == rn_old_i[1][b'samples']:\n",
    "            matches.append((j,i))\n",
    "            \n",
    "len(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1661"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches = []\n",
    "for i,tx_all_i in enumerate(tx_all):\n",
    "    for j,tx_old_i in enumerate(tx_old):\n",
    "        if tx_all_i[1][b'crossings'] == tx_old_i[1][b'crossings']:\n",
    "            matches.append((j,i))\n",
    "\n",
    "len(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1635"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spike band power old is computer with an IIR to match with nsp_implementation\n",
    "# old implementation uses mean(axis=1) and new one uses sum(axis=1)/buffer_size\n",
    "matches = []\n",
    "for i,sb_all_i in enumerate(sb_all):\n",
    "   for j,sb_old_i in enumerate(sb_old):\n",
    "       if np.all(np.isclose(np.frombuffer(sb_all_i[1][b'samples'], np.float32),\n",
    "                            np.frombuffer(sb_old_i[1][b'samples'], np.float32))):\n",
    "            matches.append((j,i))\n",
    "            \n",
    "len(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binning might not match as the alignment be off by a few bins\n",
    "matches = []\n",
    "for i,bs_all_i in enumerate(bs_all):\n",
    "   for j,bs_old_i in enumerate(bs_old):\n",
    "       if np.all(np.isclose(np.frombuffer(bs_all_i[1][b'samples'], np.float32),\n",
    "                            np.frombuffer(bs_old_i[1][b'samples'], np.float32))):\n",
    "            matches.append((j,i))\n",
    "            \n",
    "len(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched/Expected = 1635/1661\n"
     ]
    }
   ],
   "source": [
    "# verify threshold crossing and spike band power\n",
    "# Prints number of matching packets\n",
    "cnt=0\n",
    "offset = nsp_idx_all[0] - nsp_idx_old[0]\n",
    "for i in range(count-offset):\n",
    "    idx,tx_old_data =tx_old[offset+i]\n",
    "    idx,tx_all_data = tx_all[i]\n",
    "    idx,sb_old_data =sb_old[offset+i]\n",
    "    idx,sb_all_data = sb_all[i]\n",
    "\n",
    "    p_old=np.frombuffer(sb_old_data[b'samples'],dtype=np.float32)\n",
    "    p_all=np.frombuffer(sb_all_data[b'samples'],dtype=np.float32)\n",
    "    c_old=np.frombuffer(tx_old_data[b'crossings'],dtype=np.int16)\n",
    "    c_all=np.frombuffer(tx_all_data[b'crossings'],dtype=np.int16)\n",
    "\n",
    "    if   np.all(np.isclose(p_old,p_all)) and np.all(np.isclose(c_old,c_all)):cnt+=1\n",
    "if cnt:\n",
    "    print(f\"Matched/Expected = {cnt}/{count-offset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does binning from old and new threshold streams match? \n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# computes binned thresholds using old an new thresholds and compares them \n",
    "buffer_num=0\n",
    "cross_bin_buffer_old= np.zeros((256,count-offset),np.int16)\n",
    "cross_bin_buffer_all = np.zeros((256,count-offset),np.int16)\n",
    "\n",
    "for i in range(count-offset):\n",
    "    cross_bin_buffer_old[:, i] = np.frombuffer(tx_old[offset+i][1][b'crossings'],dtype=np.int16)\n",
    "    cross_bin_buffer_all[:, i] = np.frombuffer(tx_all[i][1][b'crossings'],dtype=np.int16)\n",
    "\n",
    "\n",
    "print(f\"Does binning from old and new threshold streams match? \\n{np.all(cross_bin_buffer_old.sum(axis=1) == cross_bin_buffer_all.sum(axis=1))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run node and get timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nsp_all import NSP_all\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nsp_alll] INFO: Coefficients entry found in Redis stream: rereference_parameters\n",
      "[nsp_alll] INFO: Unshuffling matrix loaded from stream.\n",
      "[nsp_alll] WARNING: 'ch_mask_stream' was set to z_mask_stream, but there were no entries. Defaulting to using all channels\n",
      "[nsp_alll] INFO: Loading 4 order, 250 hz highpass acausal IIR-IIR filter\n",
      "[nsp_alll] INFO: Loaded thresholds from the thresholds stream\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nsp_alll] Redis connection established on host: 192.168.30.6, port: 27263\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.argv = ['tx', '-n', 'nsp_alll', '-i', '192.168.30.6', '-p', '27263']\n",
    "\n",
    "params = {\n",
    "    \"input_stream\": \"nsp_neural\",\n",
    "    \"output_reref_stream\": False,\n",
    "    \"reref_maxlen\": 2000,\n",
    "    \"coefs_stream_name\": \"rereference_parameters\",\n",
    "    \"reref_stream_idx\": 0,\n",
    "    \"thresh_cross_stream_idx\": 1,\n",
    "    \"band_power_stream_idx\": 2,\n",
    "    \"binned_spikes_stream_idx\": 3,\n",
    "    \"output_streams\": [\"reref_neural_a\", \"thresh_cross_a\", \"sbp_a\", \"binned_spikes_a\"],\n",
    "    \"sync_key\": [\"tracking_id\", \"sync\", \"sync\", \"sync\"],\n",
    "    \"ts_key\": [\"BRANDS_time\", \"ts\", \"ts\", \"ts\"],\n",
    "    \"use_tracking_id\": True,\n",
    "    \"td_type\": \"uint64\",\n",
    "    \"sync_dict_key\": \"nsp_idx_1\",\n",
    "    \"filt_stream\": None,\n",
    "    \"adaptive_rms_stream\": None,\n",
    "    \"pack_per_call\": 1,\n",
    "    \"thresh_mult\": -4.5,\n",
    "    \"thresh_calc_len\": 2000,\n",
    "    \"butter_lowercut\": 250,\n",
    "    \"butter_uppercut\": None,\n",
    "    \"butter_order\": 4,\n",
    "    \"enable_CAR\": False,\n",
    "    \"output_filtered\": False,\n",
    "    \"acausal_filter_lag\": 120,\n",
    "    \"acausal_filter\": \"IIR\",\n",
    "    \"ch_mask_stream\": \"z_mask_stream\",\n",
    "    \"bandpower_logscale\": False,\n",
    "    \"chan_per_stream\": 256,  # Preserved as string since it's a reference\n",
    "    \"samp_per_stream\": 30,\n",
    "    \"chan_total\": 256,  # Preserved as string since it's a reference\n",
    "    \"start_channel\": 0,\n",
    "    \"samp_freq\": 30000,\n",
    "    \"output_dtype\": \"float32\",\n",
    "    \"total_channels\": \"*total_features\",  # Preserved as string since it's a reference\n",
    "    \"bin_size\": 10,\n",
    "    \"bin_enable\": True\n",
    "}\n",
    "\n",
    "node = NSP_all(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Timing Statistics (in milliseconds):\n",
      "--------------------------------------------------------------------------------\n",
      "Operation                            Mean        Min        Max      Count\n",
      "--------------------------------------------------------------------------------\n",
      "INIT                                8.757      8.757      8.757          1\n",
      "Redis read                          0.475      0.075      1.747      67557\n",
      "Re-referencing                      0.101      0.099      0.237      67557\n",
      "Filtering                           0.262      0.253      0.724      67553\n",
      "Threshold crossing                  0.021      0.019      0.278      67553\n",
      "Spike band power                    0.004      0.004      0.050      67553\n",
      "Total Exec Time                     0.445      0.429      0.997      67553\n",
      "Redis write                         0.132      0.113      0.683      67553\n",
      "Total Time                          1.000      0.582      2.279      67553\n",
      "Binning                             0.002      0.001      0.007       6755\n",
      "Total Binning Time                 10.000      8.972     13.939       6755\n"
     ]
    }
   ],
   "source": [
    "node.profiler.print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_streams= [\"thresh_cross_old\", \"sbp_old\"]\n",
    "stream_dict = {name.encode(): '$' for name in input_streams}\n",
    "streams = xread_sync(r,stream_dict,block=0,\n",
    "                        sync_field=b'timestamps',\n",
    "                        sync_dtype='uint64',\n",
    "                        count=10)\n",
    "in_field={b\"thresh_cross_old\":b\"crossings\", b\"sbp_old\":b\"samples\"}\n",
    "in_dtype={b\"thresh_cross_old\":np.dtype(\"int16\"), b\"sbp_old\":np.dtype(\"float32\")}\n",
    "window =np.zeros((256,10),dtype=np.float32)\n",
    "\n",
    "\n",
    "sync_entries = []\n",
    "sync_entries_stream_dump={}\n",
    "for stream in streams:\n",
    "    stream_name, stream_entries = stream\n",
    "    field = in_field[stream_name]\n",
    "    in_type = in_dtype[stream_name]\n",
    "    sync_entries_stream = []\n",
    "    synx=[]\n",
    "    for i, (entry_id, entry_dict) in enumerate(stream_entries):\n",
    "        # load the input\n",
    "        window[:, i] = np.frombuffer(entry_dict[field],\n",
    "                                            dtype=in_type).astype(\n",
    "                                                'float32')\n",
    "        # log sync for this entry\n",
    "        sync_entries_stream.append(\n",
    "            json.loads(entry_dict[b'sync'].decode()))\n",
    "        synx.append(json.loads(entry_dict[b'sync'].decode()))\n",
    "    sync_entries_stream_dump[stream_name.decode()]=synx\n",
    "    sync_entries.append(sync_entries_stream)\n",
    "    # update the xread ID\n",
    "    stream_dict[stream_name] = entry_id\n",
    "\n",
    "# create sync dict from sync entries from input streams\n",
    "sync_dict = {}\n",
    "for stream in sync_entries:\n",
    "    sync_entry_dict = stream[0]  # first entry from each stream\n",
    "    for key in sync_entry_dict:\n",
    "        sync_dict[key] = sync_entry_dict[key]\n",
    "sync_dict_json = json.dumps(sync_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "995\n"
     ]
    }
   ],
   "source": [
    "# verify threshold crossing and binned threshould_crossing\n",
    "\n",
    "cnt=0\n",
    "for i in range(1000):\n",
    "    idx,tx4_data =tx4[i]\n",
    "    idx,txa_data = txa[i]\n",
    "    idx,sb4_data =sb4[i]\n",
    "    idx,sba_data = sba[i]\n",
    "    idx,bs4_data =bs4[i]\n",
    "    idx,bsa_data = bsa[i]\n",
    "    p4=np.frombuffer(sb4_data[b'samples'],dtype=np.float32)\n",
    "    pa=np.frombuffer(sba_data[b'samples'],dtype=np.float32)\n",
    "    c4=np.frombuffer(tx4_data[b'crossings'],dtype=np.int16)\n",
    "    ca=np.frombuffer(txa_data[b'crossings'],dtype=np.int16)\n",
    "    b4=np.frombuffer(bs4_data[b'samples'],dtype=np.float32)[:256]\n",
    "    ba=np.frombuffer(bsa_data[b'samples'],dtype=np.float32)[:256]\n",
    "    if np.all(np.isclose(b4,ba)) and not np.all(np.isclose(p4,pa)) and np.all(np.isclose(c4,ca)):cnt+=1\n",
    "if cnt:\n",
    "    print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{b'sync': b'{\"nsp_idx_1\": 1000}',\n",
       " b'timestamps': b'\\xe8\\x03\\x00\\x00\\x00\\x00\\x00\\x00',\n",
       " b'crossings': b'\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00',\n",
       " b'ts': b'Q\\xba\\xdf\\xaee\\x83\\x05\\x00'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx_old_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
