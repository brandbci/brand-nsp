{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% \n",
    "# Imports\n",
    "\n",
    "import os\n",
    "import redis\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import time\n",
    "import seaborn as sns\n",
    "\n",
    "from brand.timing import timespec_to_timestamp, timeval_to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% \n",
    "# Start Redis \n",
    "\n",
    "SAVE_DIR = '/home/mrigott/Projects/emory-cart/Data/sim/2023-12-08/RawData/'\n",
    "RDB_DIR = os.path.join(SAVE_DIR,'RDB')\n",
    "RDB_FILENAME = 'sim_231208T1218_test_gemini.rdb'\n",
    "REDIS_IP = '127.0.0.1'\n",
    "REDIS_PORT = 18000\n",
    "\n",
    "redis_command = ['/home/snel/Projects/emory-cart/brand/bin/redis-server', '--bind', REDIS_IP, '--port', str(REDIS_PORT)]\n",
    "redis_command.append('--dbfilename')\n",
    "redis_command.append(RDB_FILENAME)\n",
    "redis_command.append('--dir')\n",
    "redis_command.append(RDB_DIR)\n",
    "\n",
    "print('Starting redis: ' + ' '.join(redis_command))\n",
    "\n",
    "proc = subprocess.Popen(redis_command, stdout=subprocess.PIPE)\n",
    "redis_pid = proc.pid\n",
    "\n",
    "try:\n",
    "    out, _ = proc.communicate(timeout=1)\n",
    "    if out:\n",
    "        print(out.decode())\n",
    "    if 'Address already in use' in str(out):\n",
    "        print(\"Could not run redis-server (address already in use). Check if a Redis server is already running on that port. Aborting.\")\n",
    "        exit(1)\n",
    "    else:\n",
    "        print(\"Launching redis-server failed for an unknown reason, check supervisor logs. Aborting.\")\n",
    "        exit(1)\n",
    "except subprocess.TimeoutExpired:  # no error message received\n",
    "    print('Redis-server is running.')\n",
    "\n",
    "r = redis.Redis(host=REDIS_IP, port=REDIS_PORT)\n",
    "\n",
    "busy_loading = True\n",
    "while busy_loading:\n",
    "    try:\n",
    "        print(f\"Streams in database: {r.keys('*')}\")\n",
    "        busy_loading = False\n",
    "    except redis.exceptions.BusyLoadingError:\n",
    "        print('Redis is busy loading dataset in memory')\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streams = ['nsp_neural', 'thresh_cross_1', 'sbp_1', 'binned_spikes', 'control']\n",
    "msg = 'Stream Info'\n",
    "print(msg + '\\n' + '-' * len(msg))\n",
    "for stream in streams:\n",
    "    n_entries = r.xlen(stream)\n",
    "    if n_entries > 0:\n",
    "        entry_dict = r.xrevrange(stream, count=1)[0][1]\n",
    "        has_sync = True if b'sync' in entry_dict else False\n",
    "        if has_sync:\n",
    "            entry_dict\n",
    "    else:\n",
    "        has_sync = False\n",
    "\n",
    "    row = f'{stream :24s}: {n_entries :6d}'\n",
    "    if has_sync:\n",
    "        row += f\"\\tsync={json.loads(entry_dict[b'sync'])}\"\n",
    "    else:\n",
    "        row += '\\tsync=None'\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "START = 0.0\n",
    "END = 0.2\n",
    "\n",
    "crange = np.arange(90,100,1)\n",
    "n_channels = crange.shape[0]\n",
    "\n",
    "fig, axes = plt.subplots(ncols=3,\n",
    "                         nrows=n_channels,\n",
    "                         figsize=(30, n_channels * 2),\n",
    "                         sharey='col',\n",
    "                         sharex='col',\n",
    "                         facecolor='w',\n",
    "                         tight_layout=True)\n",
    "\n",
    "for isig, sig in enumerate(['nsp_neural', 'thresh_cross_1', 'binned_spikes']):\n",
    "\n",
    "    if isig == 0:\n",
    "        N_SAMP = int((END-START)*1000)\n",
    "        N_CHANNELS = 256\n",
    "        samp_per_entry = 30\n",
    "        dtype = 'int16'\n",
    "        dfield = b'samples'\n",
    "    elif isig == 1:\n",
    "        N_SAMP = int((END-START)*1000)\n",
    "        N_CHANNELS = 256\n",
    "        samp_per_entry = 1\n",
    "        dtype = 'int16'   \n",
    "        dfield = b'crossings'    \n",
    "    elif isig == 2:\n",
    "        N_SAMP = int((END-START)*100)\n",
    "        N_CHANNELS = 512\n",
    "        samp_per_entry = 1\n",
    "        dtype = 'float32' \n",
    "        dfield = b'samples'    \n",
    "\n",
    "    stream = sig\n",
    "    entries = r.xrange(stream)\n",
    "\n",
    "    data = np.zeros((N_CHANNELS, N_SAMP*samp_per_entry), dtype=dtype)\n",
    " \n",
    "    indStart = 0\n",
    "    for i in range(N_SAMP):\n",
    "        _, entry_data = entries[i]\n",
    "        indEnd = indStart + samp_per_entry\n",
    "        samples = np.reshape(np.frombuffer(entry_data[dfield], dtype=dtype),\n",
    "                            (N_CHANNELS,samp_per_entry))\n",
    "        data[:, indStart:indEnd] = samples\n",
    "        indStart = indEnd\n",
    "\n",
    "    t = np.linspace(START, END, int(samp_per_entry*N_SAMP)) * int(samp_per_entry*N_SAMP)\n",
    "\n",
    "    for ich, ch in enumerate(crange):\n",
    "        ax = axes[ich,isig]\n",
    "        ax.plot(t, data[ch])\n",
    "        if isig == 0: ax.set_ylabel(f'Ch {ch}')\n",
    "        if ich == 0: ax.set_title(f'{sig}')\n",
    "        if ich == len(crange)-1: ax.set_xlabel('Sample #')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function\n",
    "\n",
    "def scalarfrombuffer(*args, **kwargs):\n",
    "    return np.frombuffer(*args, **kwargs)[0]\n",
    "\n",
    "def get_sync_val(sync_json, field='nsp_idx_1'):\n",
    "    sync_dict = json.loads(sync_json)\n",
    "    return sync_dict[field]\n",
    "\n",
    "streams = ['nsp_neural', 'thresh_cross_1', 'binned_spikes', 'control']\n",
    "fields = {}\n",
    "fields['sync'] = ['timestamps', 'sync', 'sync', 'sync', 'sync']\n",
    "fields['ts'] = ['BRANDS_time', 'ts', 'ts', 'ts', 'ts']\n",
    "\n",
    "dtypes = {}\n",
    "dtypes['sync'] = ['uint64', 'sync', 'sync', 'sync', 'sync']\n",
    "dtypes['ts'] = ['timespec', 'uint64', 'uint64', 'uint64', 'uint64']\n",
    "\n",
    "samp_per_entry = [30, 1, 1, 1, 1]\n",
    "\n",
    "N_CHANNELS = 256\n",
    "\n",
    "# build dataframe with data\n",
    "\n",
    "df_dict = {}\n",
    "\n",
    "for s, stream in enumerate(streams):\n",
    "    entries = r.xrange(stream)\n",
    "    data = [None] * len(entries)\n",
    "    for i, (_, entry_data) in enumerate(entries):\n",
    "        data[i] = {k: entry_data[fields[k][s].encode()] for k in fields.keys()}\n",
    "\n",
    "    if samp_per_entry[s] > 1:\n",
    "        # data2 = [None] * len(entries) * samp_per_entry[s]\n",
    "        # for i in range(len(data)):\n",
    "        #     for j in range(samp_per_entry[s]):\n",
    "        #         data2[i*samp_per_entry[s] + j] = {}\n",
    "        #         for k in data[i].keys():\n",
    "        #             byte_len = int(len(data[i][k])/samp_per_entry[s])\n",
    "        #             data2[i*samp_per_entry[s] + j][k] = data[i][k][byte_len*(j):byte_len*(j+1)]\n",
    "        data2 = [None] * len(entries)\n",
    "        for i in range(len(data)):\n",
    "            data2[i] = {}\n",
    "            for k in data[i].keys():\n",
    "                byte_len = int(len(data[i][k])/samp_per_entry[s])\n",
    "                data2[i][k] = data[i][k][0:byte_len]        \n",
    "        df = pd.DataFrame(data2)\n",
    "    else:\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "    for f in fields.keys():\n",
    "        dtype = dtypes[f][s]\n",
    "        if dtype == 'sync':\n",
    "            df[f] = df[f].apply(get_sync_val).astype(np.uint64)\n",
    "        elif dtype == 'timespec':\n",
    "            df[f] = (df[f].apply(timespec_to_timestamp) * 1e9).astype(np.uint64)\n",
    "        else:\n",
    "            df[f] = df[f].apply(scalarfrombuffer, dtype=dtype)\n",
    "\n",
    "    df_dict[stream] = df.set_index('sync', drop=True)\n",
    "\n",
    "# add suffixes to overlapping column names\n",
    "suffixes = {\n",
    "    'nsp_neural': '_ca', \n",
    "    'thresh_cross_1': '_tc', \n",
    "    'sbp_1': '_sbp', \n",
    "    'binned_spikes': '_bs', \n",
    "    'control': '_co'\n",
    "}\n",
    "\n",
    "def add_column_suffix(df, suffix=''):\n",
    "    mapper = {col: col + suffix for col in df.columns}\n",
    "    return df.rename(mapper, axis=1)\n",
    "\n",
    "\n",
    "for key in df_dict:\n",
    "    if key in suffixes:\n",
    "        df_dict[key] = add_column_suffix(df_dict[key], suffixes[key])\n",
    "    else:\n",
    "        warn(f'No suffix defined for {key} stream')\n",
    "\n",
    "df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = list(df_dict.values())[0]\n",
    "for df_i in list(df_dict.values())[1:]:\n",
    "    df = df.join(df_i)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check inter-sample intervals\n",
    "\n",
    "ts_fields = list(df.columns)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=len(ts_fields),ncols=1, figsize=(12,16), sharex=True, tight_layout=True, facecolor='w')\n",
    "\n",
    "for i, field in enumerate(ts_fields):\n",
    "    ax = axs[i]\n",
    "    isi = df[field].dropna().diff() * 1e-6\n",
    "    ax.plot(isi,'.')\n",
    "    ax.set_title(f'{field}\\n{isi.mean():2.4f} +- {isi.std():2.4f}')\n",
    "    ax.set_ylabel('ISI (ms)')\n",
    "axs[-1].set_xlabel('sync id')\n",
    "\n",
    "plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsp_step_sizes = np.unique(np.diff(df.index))\n",
    "assert len(nsp_step_sizes) == 1, (\n",
    "    'Multiple step sizes found in the NSP stream data')\n",
    "nsp_step_size = nsp_step_sizes[0]\n",
    "\n",
    "df_bin = df.dropna()\n",
    "\n",
    "bdf_binsizes = np.unique(np.diff(df_bin.index))\n",
    "assert len(bdf_binsizes) == 1, (\n",
    "    'Multiple bin sizes found in the binned stream data')\n",
    "binsize = bdf_binsizes[0]\n",
    "\n",
    "first_bin_index = df_bin.index[0]\n",
    "\n",
    "df_td = df.loc[first_bin_index:, :]\n",
    "# make a copy of the 1kHz df that has timedelta index\n",
    "df_td.index = pd.to_timedelta(df_td.index, unit='ns')\n",
    "\n",
    "resampler = df_td.resample(f'{binsize / nsp_step_size}ms')\n",
    "\n",
    "end_df = resampler.last()\n",
    "end_df.index = (end_df.index.total_seconds() * 1e9).astype(np.uint64)\n",
    "\n",
    "latency_df = end_df.diff(axis=1).iloc[:, 1:].dropna() * 1e-6\n",
    "cs_latency_df = latency_df.cumsum(axis=1)\n",
    "\n",
    "cs_latency_df.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make seaborn color palette match matplotlib\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "sns_palette = sns.color_palette(colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ts_fields = list(cs_latency_df.columns)\n",
    "\n",
    "ts_labels = {\n",
    "    'ts_ca': 'NSP Packet Reception', \n",
    "    'ts_tc': 'Feature Ext.\\n& Buffering', \n",
    "    'ts_bs': 'Binning', \n",
    "    'ts_co': 'Auto-cued\\nControl'\n",
    "}\n",
    "\n",
    "# Make plots\n",
    "fig, axes = plt.subplots(ncols=2, nrows=1, figsize=(12, 4), facecolor='w')\n",
    "labels = [ts_labels[field] for field in ts_fields[1:]]\n",
    "# plot per-node latency as a histogram\n",
    "step = 10e-3\n",
    "# for label, field in zip(labels, ts_fields[1:]):\n",
    "for label, field in zip(labels, ts_fields[1:]):\n",
    "    latency = latency_df[field].values\n",
    "    bins = np.arange(latency.max() + step, step=step)\n",
    "    axes[0].hist(latency, bins=bins, histtype='step', label=label)\n",
    "axes[0].set_xlabel('Node Latency (ms)')\n",
    "axes[0].set_ylabel('Samples')\n",
    "axes[0].set_yscale('log')\n",
    "axes[0].legend(fontsize=8,\n",
    "               ncol=1,\n",
    "               frameon=False,\n",
    "               loc='best')\n",
    "\n",
    "# plot cumulative latency as a horizontal violin plot\n",
    "sns.violinplot(data=latency_df.cumsum(axis=1),\n",
    "               scale='width',\n",
    "               linewidth=0.2,\n",
    "               orient='h',\n",
    "               palette=sns_palette,\n",
    "               ax=axes[1])\n",
    "axes[1].set_xlabel('Cumulative Latency (ms)')\n",
    "axes[1].set_yticks(ticks=np.arange(latency_df.shape[1]), labels=labels)\n",
    "\n",
    "# make the x-axes match\n",
    "ncols = len(axes)\n",
    "xlims = [axes[ip].get_xlim()[1] for ip in range(ncols)]\n",
    "for ip in range(ncols):\n",
    "    axes[ip].set_xlim(0, max(xlims))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.shutdown(nosave=True)\n",
    "\n",
    "proc.kill()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 ('rt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d2128b57a97265db2d26af59885552af5b27103cf6f1a22713256a0616a427a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
